filtered = filter(user_yob2_unknown, category_no == i)
user_yob2_unknown[filtered$id_numeric, ]$category = categories_yob$category[i]
filtered                                          = data.frame()
}
)
unknown1$yob_cat = factor(user_yob2_unknown$category,
levels = c("NA", "< 1940", "1940 - 1949", "1950 - 1959",
"1960 - 1969", "1970 - 1979", "1980 - 1989",
"1990 - 1999", "2000 - 2009", "> 2009"))
rm(user_yob2_unknown); rm(user_yob2); rm(categories_yob); rm(filtered)
rm(na_user_yob2); rm(yob_cat)
#extracting birth_day
known1$birth_day                                 = weekdays(as.Date(known1$user_dob))
known1$birth_day[which(is.na(known1$birth_day))] = "missing"
known1$birth_day                                 = as.factor(known1$birth_day)
birth_day             = data.frame(table(known1$birth_day))
colnames(birth_day)   = c("weekday", "freq")
birth_day$returns     = aggregate(as.numeric(known1$return),
by = list(Category = known1$birth_day),
FUN = sum)[,2]
birth_day$return_prob = birth_day$returns/birth_day$freq
known1$birth_day      = NULL
rm(birth_day)
#extracting birth_month
known1$birth_month                                   = format(known1$user_dob, "%m")
known1$birth_month[which(is.na(known1$birth_month))] = "missing"
known1$birth_month                                   = as.factor(known1$birth_month)
birth_month             = data.frame(table(known1$birth_month))
colnames(birth_month)   = c("month", "freq")
birth_month$returns     = aggregate(as.numeric(known1$return),
by = list(Category = known1$birth_month),
FUN = sum)[,2]
birth_month$return_prob = birth_month$returns/birth_month$freq
known1$birth_month      = NULL
rm(birth_month)
#extracting birth_quarter
known1$birth_quarter = substr(quarters(known1$user_dob), 2, 2)
known1$birth_quarter[which(known1$birth_quarter == "N")] = "missing"
known1$birth_quarter = as.factor(known1$birth_quarter)
levels(known1$birth_quarter)
birth_quarter             = data.frame(table(known1$birth_quarter))
colnames(birth_quarter)   = c("quarter", "freq")
birth_quarter$returns     = aggregate(as.numeric(known1$return),
by = list(Category = known1$birth_quarter),
FUN = sum)[,2]
birth_quarter$return_prob = birth_quarter$returns/birth_quarter$freq
known1$birth_quarter      = NULL
rm(birth_quarter)
#extracting month day of birth (numeric)
known1$birth_monthday = format(known1$user_dob, "%d")
known1$birth_monthday[which(is.na(known1$birth_monthday))] = "missing"
known1$birth_monthday = as.factor(known1$birth_monthday)
birth_monthday             = data.frame(table(known1$birth_monthday))
colnames(birth_monthday)   = c("monthday", "freq")
birth_monthday$returns     = aggregate(as.numeric(known1$return),
by = list(Category = known1$birth_monthday),
FUN = sum)[,2]
birth_monthday$return_prob = birth_monthday$returns/birth_monthday$freq
cor(as.numeric(known1$birth_monthday), known1$return)
knownc = known1[-which(known1$birth_monthday == "missing"), ]
cor(knownc$return, as.numeric(knownc$birth_monthday))
rm(knownc)
rm(birth_monthday)
known1$birth_monthday = NULL
#birth dates as factor
known1$dob_factor         = as.factor(known1$user_dob)
levels(known1$dob_factor) = c(as.character(levels(known1$dob_factor)), "missing")
known1$dob_factor[which(is.na(known1$dob_factor))] = "missing"
dob_factor             = data.frame(table(known1$dob_factor))
colnames(dob_factor)   = c("birth_date", "freq")
dob_factor$returns     = aggregate(as.numeric(known1$return),
by = list(Category = known1$dob_factor),
FUN = sum)[,2]
dob_factor$return_prob = dob_factor$returns/dob_factor$freq
known1$dob_factor      = NULL
rm(dob_factor)
```
The data set contains also id numbers for items, brands and users. IDs are hard to use for predictive modelling, since they contain very few information about the actual character of the represented entity. For all three id columns, the values were included as integer values, since using these values as factors would lead to a high number of factor levels, what makes model calculation very time consuming. Using the weight of evidence in this case seems to be an appropriate measure to cope with the vast amount of factor levels. But including the weight of evidence in the prediction models yielded in very poor accuracy (probably due to overfitting). The integer values could maybe include some hidden patterns in the assignment of id values (e.g. higher *item_id* values correspond to a more recent item). Another piece of information that could be extracted from the id numbers, was the frequency of occurrence. These variables represent which items and brands are frequently bought and which users order items frequently.
```{r, eval=FALSE}
## item_id
#id as integer
known1$item_id   = as.integer(known$item_id)
unknown1$item_id = as.integer(unknown$item_id)
cor(known1$item_id, known1$return)
# frequency of item_id
i_id1               = known1 %>% add_count(item_id)
known1$item_id_freq = i_id1$n
rm(i_id1)
i_id2                 = unknown1 %>% add_count(item_id)
unknown1$item_id_freq = i_id2$n
rm(i_id2)
## brand_id
#include brand_id as integer value
known1$brand_id   = as.integer(known$brand_id)
unknown1$brand_id = as.integer(unknown$brand_id)
#include brand_id frequencies
b_id1                = known1 %>% add_count(as.factor(brand_id))
known1$brand_id_freq = b_id1$n
rm(b_id1)
b_id2                  = unknown1 %>% add_count(as.factor(brand_id))
unknown1$brand_id_freq = b_id2$n
rm(b_id2)
## user_id
#include user_id as integer value
known1$user_id   = as.integer(known$user_id)
unknown1$user_id = as.integer(unknown$user_id)
#include brand_id frequencies
u_id1               = known1 %>% add_count(as.factor(user_id))
known1$user_id_freq = u_id1$n
rm(u_id1)
u_id2                 = unknown1 %>% add_count(as.factor(user_id))
unknown1$user_id_freq = u_id2$n
rm(u_id2)
```
The variable *item_size* is a factor with a high number of levels, that should be reduced as much as possible. This was achieved by transforming different descriptions of the same size to the same level. Another problem was to ensure, that there were no factor levels in the unknown data set, that are not included in the known data set for training. This was achieved by including the category "other". Since only few observations were affected, this action had no major influence on the data. Similarly to the id variables, the frequencies of occurrence were also included for the item size.
```{r, eval = FALSE}
## item_size
known1$item_size   = as.factor(known$item_size)
unknown1$item_size = as.factor(unknown$item_size)
item_size             = data.frame(table(known1$item_size))
colnames(item_size)   = c("item_size", "freq")
item_size$returns     = aggregate(as.numeric(known1$return),
by = list(Category = known1$item_size),
FUN = sum)[,2]
item_size$return_prob = item_size$returns/item_size$freq
item_size_unknown     = data.frame(table(unknown1$item_size))
#comparing levels
size_levels_k = data.frame("levels" = as.character(levels(known1$item_size)))
size_levels_u = data.frame("levels" = as.character(levels(unknown1$item_size)))
size_levels_k$in_unknown = size_levels_k$levels %in% size_levels_u$levels
size_levels_u$in_known   = size_levels_u$levels %in% size_levels_k$levels
#Due to the large number of different item size facor levels, it is necessary to match
#the levels of the unknown and the known data set, in order to generate useful
#predictions.
#change levels to "other": c("105", "2+", "2932", "58", "76", "80", "95", "3132", "4034",
#"49") sum: 23 observations
known1$item_size = as.character(known1$item_size)
known1$item_size[known1$item_size %in% c("105", "2+", "2932", "58", "76",
"80", "95", "3132", "4034", "49")] = "other"
known1$item_size = as.factor(known1$item_size)
unknown1$item_size = as.character(unknown1$item_size)
unknown1$item_size[unknown1$item_size %in% c("105", "2+", "2932", "58", "76",
"80", "95", "3132", "4034", "49")] = "other"
unknown1$item_size = as.factor(unknown1$item_size)
#check result
size_levels_k = data.frame("levels" = as.character(levels(known1$item_size)))
size_levels_u = data.frame("levels" = as.character(levels(unknown1$item_size)))
size_levels_k$in_unknown = size_levels_k$levels %in% size_levels_u$levels
size_levels_u$in_known   = size_levels_u$levels %in% size_levels_k$levels
rm(item_size); rm(item_size_unknown); rm(size_levels_k); rm(size_levels_u)
#combine same sizes with different names
old_sizes = c('m','M','l','L','xl','XL','43+','37+','36+','XXL','xxl','s','S','XS','xs',
'XXXL','xxxl','8+','8','6+','6','9+','9','4','4+','2','2+','7','7+','5',
'5+','39+','38+','10','10+','11','11+','40+','21','20','42+','41+','12+',
'3+','45+','44+','46+')
new_sizes = c(38,38,42,42,46,46,43,37,36,50,50,36,36,34,34,54,54,36,36,34,34,38,38,32,
32,30,30,36,36,33,33,39,38,38,38,39,39,40,48,48,42,41,12,3,45,44,46)
size_df = data.frame(unique(rbind(
data.frame("size" = levels(as.factor(known1$item_size))),
data.frame("size" = levels(as.factor(unknown1$item_size))))))
size_df$numeric  = as.numeric(as.character(size_df$size))
size_df$new_size = size_df$size
size_df$id_num   = seq(1:nrow(size_df))
for( i in 1:length(old_sizes)) {
size_df$new_size[which(size_df$size == old_sizes[i])] = new_sizes[i]
}
size_df$new_size = as.character(size_df$new_size)
#create item_size2 column in known1
known1$item_size2 = 0
filtered          = data.frame()
for(i in 1:nrow(size_df)) {
filtered = known1 %>%
filter(item_size == size_df$size[i])
known1$item_size2[filtered$order_item_id] = size_df$new_size[i]
filtered                                  = data.frame()
}
known1$item_size2 = as.factor(known1$item_size2)
head(known1$item_size, 20)
head(known1$item_size2, 20)
#unknown
unknown1$item_size2 = 0
filtered            = data.frame()
for(i in 1:nrow(size_df)) {
filtered = unknown1 %>%
filter(item_size == size_df$size[i])
unknown1$item_size2[filtered$order_item_id] = size_df$new_size[i]
filtered                                    = data.frame()
}
unknown1$item_size2 = as.factor(unknown1$item_size2)
rm(size_df); rm(filtered); rm(new_sizes); rm(old_sizes)
#checking for level equality
size_levels_k = data.frame("levels" = as.character(levels(known1$item_size2)))
size_levels_u = data.frame("levels" = as.character(levels(unknown1$item_size2)))
size_levels_k$in_unknown = size_levels_k$levels %in% size_levels_u$levels
size_levels_u$in_known   = size_levels_u$levels %in% size_levels_k$levels
rm(size_levels_k); rm(size_levels_u)
# add frequency of item size
s_id1                 = known1 %>% add_count(item_size)
known1$item_size_freq = s_id1$n
rm(s_id1)
s_id2                   = unknown1 %>% add_count(item_size)
unknown1$item_size_freq = s_id2$n
rm(s_id2)
```
The item colour had also many factor levels that had to be reduced. Therefore, the colour names were aggregated to some base colour categories. This reduced the number of factor levels dramatically. Again, a category "other" was included to ensure equal factor levels in the known and the unknown data and to assign ambiguous colour labels. Similar to the item size, the frequencies of the original colour name occurrences were also included.
```{r, eval=FALSE}
## item_color
# checking for level equality
known1$item_color   = as.factor(known$item_color)
unknown1$item_color = as.factor(unknown$item_color)
item_color             = data.frame(table(known1$item_color))
colnames(item_color)   = c("item_color", "freq")
item_color$returns     = aggregate(as.numeric(known1$return),
by = list(Category = known1$item_color),
FUN = sum)[,2]
item_color$return_prob = item_color$returns/item_color$freq
item_color_unknown     = data.frame(table(unknown1$item_color))
#comparing levels
color_levels_k = data.frame("levels" = as.character(levels(known1$item_color)))
color_levels_u = data.frame("levels" = as.character(levels(unknown1$item_color)))
color_levels_k$in_unknown = color_levels_k$levels %in% color_levels_u$levels
color_levels_u$in_known   = color_levels_u$levels %in% color_levels_k$levels
#change levels to other: c("amethyst", "opal", "perlmutt", "vanille", "cortina mocca")
#sum: 7 observations
known1$item_color = as.character(known1$item_color)
known1$item_color[known1$item_color %in% c("amethyst", "opal", "perlmutt", "vanille",
"cortina mocca")] = "other"
known1$item_color = as.factor(known1$item_color)
unknown1$item_color = as.character(unknown1$item_color)
unknown1$item_color[unknown1$item_color %in% c("amethyst", "opal", "perlmutt", "vanille",
"cortina mocca")] = "other"
unknown1$item_color = as.factor(unknown1$item_color)
#check result
color_levels_k = data.frame("levels" = as.character(levels(known1$item_color)))
color_levels_u = data.frame("levels" = as.character(levels(unknown1$item_color)))
color_levels_k$in_unknown = color_levels_k$levels %in% color_levels_u$levels
color_levels_u$in_known   = color_levels_u$levels %in% color_levels_k$levels
rm(item_color); rm(item_color_unknown); rm(color_levels_k); rm(color_levels_u)
#aggregate different color levels to base colors
all_color_levels = data.frame(unique(rbind(
data.frame("color" = levels(as.factor(known1$item_color))),
data.frame("color" = levels(as.factor(unknown1$item_color))))))
color_check = function(x){
if(x %in% c("avocado", "jade","mint","khaki","oliv", "dark oliv","olive", "green")){
return("green")
}else if(x %in% c("opal", "cobalt blue","baltic blue", "dark navy","darkblue",
"aqua","blau", "navy", "azure", "dark denim","aquamarine","denim",
"turquoise", "blue", "petrol")){
return("blue")
}else if(x %in% c("perlmutt", "white")){
return("white")
}else if(x %in% c("amethyst", "currant purple" , "pallid", "magenta", "aubergine",
"berry", "purple")){
return("purple")
}else if(x %in% c("crimson", "dark garnet", "bordeaux","apricot","coral","mango",
"terracotta", "red")){
return("red")
}else if(x %in% c("apricot","coral","mango", "orange")){
return("orange")
}else if(x %in% c("lemon", "vanille","curry", "yellow", "gold")){
return("yellow")
}else if(x %in% c("aviator", "graphite", "ash", "anthracite","dark grey",
"basalt", "iron", "grey", "silver")){
return("grey")
}else if(x %in% c("copper coin", "caramel", "brwon", "kanel", "habana",
"mahagoni", "cognac", "mocca","cortina mocca", "brown")){
return("brown")
}else if(x %in% c("ebony", "black")){
return("black")
}else if(x %in% c("antique pink", "fuchsia", "pink", "hibiscus")){
return("pink")
}else if(x %in% c("creme","almond", "champagner", "ivory", "ingwer","ecru",
"beige", "ocher")){
return("beige")
}else if(x %in% c("?","ancient","curled","floral","nature", "striped", "stained",
"other")){
return("other")
}else{
return("other")
}
}
#for known and unknown
known1$item_color2   = sapply(known1$item_color, FUN = color_check)
known1$item_color2   = as.factor(known1$item_color2)
unknown1$item_color2 = sapply(unknown1$item_color, FUN = color_check)
unknown1$item_color2 = as.factor(unknown1$item_color2)
head(known1$item_color2, 20); head(known1$item_color, 20)
head(unknown1$item_color2, 20); head(unknown1$item_color, 20)
#check again for level equality
#comparing levels
color_levels_k = data.frame("levels" = as.character(levels(known1$item_color2)))
color_levels_u = data.frame("levels" = as.character(levels(unknown1$item_color2)))
color_levels_k$in_unknown = color_levels_k$levels %in% color_levels_u$levels
color_levels_u$in_known   = color_levels_u$levels %in% color_levels_k$levels
rm(color_levels_k); rm(color_levels_u); rm(all_color_levels); rm(color_check)
#include frequencies of colors
c_id1                  = known1 %>% add_count(item_color)
known1$item_color_freq = c_id1$n
rm(c_id1)
c_id2                    = unknown1 %>% add_count(item_color)
unknown1$item_color_freq = c_id2$n
rm(c_id2)
```
The item prices were included unchanged as numeric values for building the prediction models.
```{r, eval = FALSE}
#item_price
#include item_price as numeric variable
known1$item_price   = as.numeric(known$item_price)
unknown1$item_price = as.numeric(unknown$item_price)
```
The user title was included in two versions. First, using the original four factor levels from the genuine data set. The second representation consists of a binary variable, indicating whether a user is female or not. Since almost 96% of the users are female, merging the remaining factor levels might be useful.
```{r, eval=FALSE}
## user_title
#include user_title as factor
known1$user_title   = as.factor(known$user_title)
unknown1$user_title = as.factor(unknown$user_title)
# compare titles
user_title             = data.frame(table(known1$user_title))
colnames(user_title)   = c("title", "freq")
user_title$returns     = aggregate(as.numeric(known1$return),
by = list(Category = known1$user_title),
FUN = sum)[,2]
user_title$return_prob = user_title$returns/user_title$freq
rm(user_title)
#include binary variable for female
known1$female                                    = 0
known1$female[which(known1$user_title == "Mrs")] = 1
unknown1$female                                      = 0
unknown1$female[which(unknown1$user_title == "Mrs")] = 1
```
Similar to the user title, the user state was also included in two different ways. Once in the original version including all states and second, again as a binary variable indicating whether a customer is from the eastern part of Germany.
```{r, eval=FALSE}
## user_state
known1$user_state   = as.factor(known$user_state)
unknown1$user_state = as.factor(unknown$user_state)
user_states             = data.frame(table(known1$user_state))
colnames(user_states)   = c("user_state", "freq")
user_states$returns     = aggregate(as.numeric(known1$return),
by = list(Category = known1$user_state),
FUN = sum)[,2]
user_states$return_prob = user_states$returns/user_states$freq
rm(user_states)
#include binary variable for east and west germany
east = c("Brandenburg", "Mecklenburg-Western Pomerania", "Saxony", "Thuringia",
"Saxony-Anhalt")
known1$east                                     = 0
known1$east[which(known1$user_state %in% east)] = 1
unknown1$east                                       = 0
unknown1$east[which(unknown1$user_state %in% east)] = 1
rm(east)
```
After the feature engineering part was completed, the resulting data frames for known and unknown data were saved as .csv files, in order to save time when using the created variables again. Furthermore, it was also necessary to ensure the correct data type of all columns in the data frames. The date variables were converted to integer values, since most algorithms are not able to cope with the date format.
```{r, eval = FALSE, message=FALSE}
## return
#set return to last position
known1$return = NULL
known1$return = known$return
##compare colums of known1 and unknown1
columns = cbind(colnames(known1), c(colnames(unknown1),"0"))
rm(columns)
##save files
write.csv(known1, file = "known1.csv", row.names = FALSE)
write.csv(unknown1, file = "unknown1.csv", row.names = FALSE)
#read saved files
library(readr)
wd = "C:/Users/timpe_000/Desktop/BADS_assignment_tim_peschenz"
setwd(wd)
known   = data.frame(read_csv("BADS_WS1819_known.csv"))
unknown = data.frame(read_csv("BADS_WS1819_unknown.csv"))
known1   = data.frame(read_csv("known1.csv"))
unknown1 = data.frame(read_csv("unknown1.csv"))
##change all columns to correct data type
#specify factor columns
factor_cols = c("order_month", "order_quarter", "order_year", "delivery_date_missing",
"delivery_time_negative", "delivery_month", "delivery_quarter",
"delivery_year", "item_size", "item_size2", "item_color",
"item_color2", "user_title", "female", "user_dob_missing", "yob_cat",
"user_state", "east","reg_month", "reg_quarter", "reg_year")
known1[factor_cols] = lapply(known1[factor_cols], as.factor)
unknown1[factor_cols] = lapply(unknown1[factor_cols], as.factor)
known1$return = as.factor(known$return)
#unclass date columns
date_cols = c("order_date", "delivery_date", "user_dob", "user_reg_date")
known1[date_cols] = lapply(known1[date_cols], unclass)
unknown1[date_cols] = lapply(unknown1[date_cols], unclass)
known1$yob_cat = addNA(known1$yob_cat)
unknown1$yob_cat = addNA(unknown1$yob_cat)
```
The last preparation step was to split the data into a test set and multiple training sets. The idea behind using multiple training sets was to use the different data inputs for the creation of diverse ensembles for the model library (Tsoumakas, et al., 2009). But since this approach yielded in poor results, the rows of the input data were not changed among the various model buildings. Knowing that the observations with missing delivery dates always result in non-returns, it is sensible to exclude all these observations for the model building process. Including these rows would probably yield to biased predictions. Therefore, *known1* and *unknown1* were transformed to the *knownc* and *unknownc* data frames, which included solely the observations with non-missing delivery dates. Instead of using different observations as input data, the available features were manually separated into three different feature sets, that should induce diversity among the prediction models in the library (Dietterich, 2000).
```{r, eval = FALSE}
##splitting data frame into several tranings and test set
#create numbers for different set memberships
set.seed(26)
known1$set_id = sample(c(1, 2, 3, 4), size = nrow(known1),
replace = TRUE, prob = c(0.2, 0.2, 0.2, 0.4))
#create different training sets
train1 = known1[known1$set_id == 1, ]
train1 = train1[complete.cases(train1$delivery_date),]
train2 = known1[known1$set_id == 2, ]
train2 = train2[complete.cases(train2$delivery_date),]
train3 = known1[known1$set_id == 3, ]
train3 = train3[complete.cases(train3$delivery_date),]
train  = list(train1, train2, train3)
#training set with all complete cases
trainc = known1[known1$set_id != 4, ]
trainc = trainc[complete.cases(trainc$delivery_date),]
#create test set
test  = known1[known1$set_id == 4, ]
test$test_id = 1:nrow(test)
testc = test[complete.cases(test$delivery_date), ]
test_id = testc$test_id
##create different feature sets
colnames(known1)
features = list(c("order_date", "order_month", "item_id", "item_color_freq",
"item_price", "user_id", "female", "yob_cat", "east", "return"),
c("delivery_date", "delivery_time", "delivery_month", "item_id_freq",
"brand_id", "brand_id_freq", "item_price", "user_title", "age",
"return"),
c("item_size_freq", "item_color2", "item_price", "user_id_freq",
"user_dob_missing", "user_state", "user_reg_date", "reg_month",
"reg_year", "return"))
head(trainc[, features[[1]]])
#count nas
na_count = sapply(trainc, function(x) sum(is.na(x)))
#replace missing values for delivery time by 0
trainc$delivery_time[which(is.na(trainc$delivery_time))] = 0
testc$delivery_time[which(is.na(testc$delivery_time))]   = 0
#data frames with all rows without missing delivery date
knownc = known1[complete.cases(known1$delivery_date), ]
unknown1$unknown_id = 1:nrow(unknown1)
unknownc = unknown1[complete.cases(unknown1$delivery_date), ]
unknown_id = unknownc$unknown_id
```
To examine the importance of the created features, a random forest model was trained, using the ranger R package. This offered an overview and useful orientation point for the selection of explaining variables for the prediction models.
```{r eval=FALSE}
#examine variable importance using random forest model
library(ranger)
knownc               = known1[complete.cases(known1), ]
knownc$order_item_id = NULL
knownc$set_id        = NULL
rf_var_imp = ranger(return ~ ., data = knownc,
importance = "impurity",
num.trees = 1000,
probability = TRUE)
var_imp           = data.frame(rf_var_imp$variable.importance)
colnames(var_imp) = c("importance")
#extract variables with comparably low importance
rownames(var_imp)[which(var_imp$importance < 500)]
#extract most important variables
rownames(var_imp)[which(var_imp$importance > 1000)]
#barplot of variable importance
library(ggplot2)
ggplot(data=var_imp, aes(x=rownames(var_imp), y=importance)) +
geom_bar(stat="identity") +
theme(axis.text.x = element_text(angle = 90, hjust = 1)) +
xlab("varible")
```
```{r, fig.align='center', fig.cap='Variable Importance'}
knitr::include_graphics('var_imp.png')
```
## Model tuning, selection and evaluation
After finishing the data preparation and examining the variable importance, a gradient boosting model was trained using the *xgboost* package. Several features were selected, oriented on the random forest feature importance values. To use the factor features with the *xgboost* package, it was necessary to create dummy features of the factor levels. Parameter tuning was done by running a random parameter search using the *mlr* package. A grid search would possibly find a better parameter constellation. The decision for a random search was done due to time and computational constraints. For the model development, only complete observations were used, in order not to distort the prediction results. The model was trained using the best parameter values from the random search. Afterwards, the predictions for the observations of the test set without missing delivery dates were calculated. Those instances with a missing delivery date were predicted to be zero, assuming that this is a general "rule" for this data set. The predictive power was assessed by calculating the AUC of the ROC, yielding at a value of approximately 0.71.
```{r, eval = FALSE}
### gradient boosting model ###
#loading packages
library(caret)
library(mlr)
#select features that are used in the prediction model
feature_cols = c("order_date", "order_month", "order_quarter", "delivery_date",
"delivery_time", "item_id","item_id_freq", "item_size2", "item_color2",
"brand_id", "brand_id_freq", "item_price","user_id", "user_id_freq",
"user_title", "age", "user_state", "user_reg_date", "return")
trainc = trainc[feature_cols]
testc  = testc[feature_cols]
# Prepare the mlr task
# create dummy variables to use the factors
train_dummy = mlr::createDummyFeatures(trainc, target = "return")
test_dummy  = mlr::createDummyFeatures(testc, target = "return")
#comparing columns
col_check                 = data.frame(colnames(test_dummy))
col_check$in_knownc_dummy = sapply(col_check[,1], function(x){
ifelse(x %in% colnames(train_dummy), 1, 0)})
#mlr task
task = makeClassifTask(data = train_dummy, target = "return", positive = "1")
#loading xgboost package
library("xgboost")
xgb.learner = makeLearner("classif.xgboost", predict.type = "prob",
par.vals = list("verbose" = 0,
"early_stopping_rounds"=10))
# Set tuning parameters
xgb.parms = makeParamSet(
makeNumericParam("eta", lower = 0.01, upper = 0.1),
makeIntegerParam("nrounds", lower=40, upper=300),
makeIntegerParam("max_depth", lower=2, upper=6),
makeDiscreteParam("gamma", values = 0),
makeDiscreteParam("colsample_bytree", values = 1),
makeDiscreteParam("min_child_weight", values = 1),
makeDiscreteParam("subsample", values = 0.9)
)
# Random Parameter Tuning
tuneControl = makeTuneControlRandom(maxit=10, tune.threshold = FALSE)
# 3 fold cross validation
rdesc = makeResampleDesc(method = "CV", iters = 3, stratify = TRUE)
library("parallelMap")
library(parallel)
parallelStartSocket(3, level = "mlr.tuneParams")
set.seed(123)
RNGkind("L'Ecuyer-CMRG")
clusterSetRNGStream(iseed = 1234567)
# Tune parameters
xgb.tuning = tuneParams(xgb.learner, task = task, resampling = rdesc,
par.set = xgb.parms, control = tuneControl, measures = mlr::auc)
